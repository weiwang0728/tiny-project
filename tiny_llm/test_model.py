import os
import sys
import torch

# ç¡®ä¿èƒ½å¯¼å…¥ model æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from model import Transformer, ModelArgs
from tokenizer import Tokenizer


def test_generate_function():
    """æµ‹è¯• Transformer ç±»çš„ generate å‡½æ•°"""
    print("=== æµ‹è¯• Transformer.generate å‡½æ•° ===")
    
    try:
        # åˆå§‹åŒ–æ¨¡å‹å‚æ•°
        args = ModelArgs(
            dim=128,          # å‡å°ç»´åº¦ä»¥åŠ å¿«æµ‹è¯•é€Ÿåº¦
            n_layers=2,       # å‡å°‘å±‚æ•°
            n_heads=16,        # å‡å°‘å¤´æ•°
            n_kv_heads=8,
            vocab_size=4096,  # ä½¿ç”¨è¾ƒå°çš„è¯æ±‡è¡¨
            max_seq_len=64    # å‡å°æœ€å¤§åºåˆ—é•¿åº¦
        )
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("1. åˆå§‹åŒ–æ¨¡å‹...")
        model = Transformer(args)
        print(f"âœ“ æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ! å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # åˆå§‹åŒ–åˆ†è¯å™¨
        print("\n2. åˆå§‹åŒ–åˆ†è¯å™¨...")
        tokenizer = Tokenizer("data/tok4096.model")
        print(f"âœ“ åˆ†è¯å™¨åˆå§‹åŒ–æˆåŠŸ! è¯æ±‡è¡¨å¤§å°: {tokenizer.n_words}")
        
        # å‡†å¤‡è¾“å…¥æ–‡æœ¬å’Œtoken
        print("\n3. å‡†å¤‡æµ‹è¯•æ•°æ®...")
        test_text = "Once upon a time, there was a little girl named"
        test_tokens = tokenizer.encode(test_text, bos=True, eos=False)
        # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        input_tensor = torch.tensor([test_tokens], dtype=torch.long)
        print(f"âœ“ è¾“å…¥æ–‡æœ¬: '{test_text}'")
        print(f"âœ“ è¾“å…¥tokenæ•°é‡: {len(test_tokens)}")
        
        # æµ‹è¯•åŸºæœ¬ç”ŸæˆåŠŸèƒ½
        print("\n4. æµ‹è¯•åŸºæœ¬ç”ŸæˆåŠŸèƒ½...")
        max_new_tokens = 10
        generated_tokens = model.generate(input_tensor, max_new_tokens)
        print(f"âœ“ ç”ŸæˆæˆåŠŸ! ç”Ÿæˆçš„tokenå½¢çŠ¶: {generated_tokens.shape}")
        assert generated_tokens.shape[1] == len(test_tokens) + max_new_tokens, \
            f"æœŸæœ›è¾“å‡ºé•¿åº¦ä¸º {len(test_tokens) + max_new_tokens}ï¼Œå®é™…ä¸º {generated_tokens.shape[1]}"
        print("âœ“ è¾“å‡ºé•¿åº¦éªŒè¯é€šè¿‡!")
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = tokenizer.decode(generated_tokens[0].tolist())
        print(f"âœ“ ç”Ÿæˆçš„æ–‡æœ¬: '{generated_text}'")
        
        # æµ‹è¯•ä¸åŒçš„temperatureå€¼
        print("\n5. æµ‹è¯•ä¸åŒçš„temperatureå€¼...")
        temperatures = [0.0, 0.5, 1.0, 2.0]
        
        for temp in temperatures:
            print(f"  æµ‹è¯• temperature={temp}...")
            with torch.no_grad():
                gen_tokens_temp = model.generate(input_tensor, max_new_tokens, temperature=temp)
                gen_text_temp = tokenizer.decode(gen_tokens_temp[0].tolist())
                print(f"    ç”Ÿæˆç»“æœ: '{gen_text_temp}'")
        
        # æµ‹è¯•ä¸åŒçš„top_kå€¼
        print("\n6. æµ‹è¯•ä¸åŒçš„top_kå€¼...")
        top_k_values = [1, 5, 20, None]
        
        for top_k in top_k_values:
            print(f"  æµ‹è¯• top_k={top_k}...")
            with torch.no_grad():
                gen_tokens_topk = model.generate(input_tensor, max_new_tokens, temperature=0.7, top_k=top_k)
                gen_text_topk = tokenizer.decode(gen_tokens_topk[0].tolist())
                print(f"    ç”Ÿæˆç»“æœ: '{gen_text_topk}'")
        
        # æµ‹è¯•è¾¹ç•Œæƒ…å†µï¼šåºåˆ—é•¿åº¦é™åˆ¶
        print("\n7. æµ‹è¯•åºåˆ—é•¿åº¦é™åˆ¶...")
        # åˆ›å»ºä¸€ä¸ªæ¥è¿‘æœ€å¤§åºåˆ—é•¿åº¦çš„è¾“å…¥
        long_input = torch.randint(0, args.vocab_size, (1, args.max_seq_len - 5), dtype=torch.long)
        # ç”Ÿæˆè¶³å¤Ÿçš„tokenï¼Œä½¿å…¶è¶…è¿‡æœ€å¤§åºåˆ—é•¿åº¦
        with torch.no_grad():
            gen_tokens_long = model.generate(long_input, 20)
        print(f"âœ“ é•¿åºåˆ—ç”ŸæˆæˆåŠŸ! è¾“å…¥é•¿åº¦: {long_input.shape[1]}, è¾“å‡ºé•¿åº¦: {gen_tokens_long.shape[1]}")
        
        print("\n8. æµ‹è¯•é›¶æ¸©åº¦ç”Ÿæˆï¼ˆç¡®å®šæ€§è¾“å‡ºï¼‰...")
        # é›¶æ¸©åº¦ä¸‹åº”è¯¥æ¯æ¬¡ç”Ÿæˆç›¸åŒçš„ç»“æœ
        with torch.no_grad():
            gen1 = model.generate(input_tensor, max_new_tokens, temperature=0.0)
            gen2 = model.generate(input_tensor, max_new_tokens, temperature=0.0)
        
        # æ£€æŸ¥ä¸¤æ¬¡ç”Ÿæˆçš„ç»“æœæ˜¯å¦ç›¸åŒ
        same_result = torch.allclose(gen1, gen2)
        print(f"âœ“ é›¶æ¸©åº¦ç”Ÿæˆç»“æœä¸€è‡´æ€§: {'é€šè¿‡' if same_result else 'å¤±è´¥'}")
        
        print("\nğŸ‰ Transformer.generate å‡½æ•°æµ‹è¯•å…¨éƒ¨é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"Transformer.generate å‡½æ•°æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_generate_edge_cases():
    """æµ‹è¯• Transformer.generate å‡½æ•°çš„è¾¹ç¼˜æƒ…å†µ"""
    print("\n=== æµ‹è¯• Transformer.generate å‡½æ•°çš„è¾¹ç¼˜æƒ…å†µ ===")
    
    try:
        # åˆå§‹åŒ–ç®€åŒ–çš„æ¨¡å‹å‚æ•°ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•
        args = ModelArgs(
            dim=128,          # æ›´å°çš„ç»´åº¦
            n_layers=1,      # åªæœ‰1å±‚
            n_heads=16,    
            n_kv_heads=8,
            vocab_size=4096, 
            max_seq_len=64   
        )
        print("1",args.n_heads)
        print("2",args.n_kv_heads)
        # åˆå§‹åŒ–æ¨¡å‹
        model = Transformer(args)
        
        # åˆå§‹åŒ–åˆ†è¯å™¨
        tokenizer = Tokenizer("data/tok4096.model")
        
        # æµ‹è¯•1: ç©ºè¾“å…¥ï¼ˆåªæœ‰BOS tokenï¼‰
        print("\n1. æµ‹è¯•ç©ºè¾“å…¥ï¼ˆåªæœ‰BOS tokenï¼‰...")
        bos_token = tokenizer.bos_id
        bos_input = torch.tensor([[bos_token]], dtype=torch.long)
        with torch.no_grad():
            gen_from_bos = model.generate(bos_input, 10)
        gen_text_bos = tokenizer.decode(gen_from_bos[0].tolist())
        print(f"âœ“ ä»BOS tokenç”Ÿæˆçš„æ–‡æœ¬: '{gen_text_bos}'")
        
        # æµ‹è¯•2: ç”Ÿæˆå¤§é‡token
        print("\n2. æµ‹è¯•ç”Ÿæˆå¤§é‡token...")
        short_input = torch.randint(0, args.vocab_size, (1, 3), dtype=torch.long)
        large_max_tokens = 50
        with torch.no_grad():
            gen_large = model.generate(short_input, large_max_tokens)
        print(f"âœ“ ç”Ÿæˆå¤§é‡tokenæˆåŠŸ! æ€»é•¿åº¦: {gen_large.shape[1]}")
        assert gen_large.shape[1] == 3 + large_max_tokens, \
            f"æœŸæœ›é•¿åº¦ä¸º {3 + large_max_tokens}ï¼Œå®é™…ä¸º {gen_large.shape[1]}"
        
        # æµ‹è¯•3: æ‰¹é‡è¾“å…¥
        print("\n3. æµ‹è¯•æ‰¹é‡è¾“å…¥...")
        batch_input = torch.randint(0, args.vocab_size, (2, 5), dtype=torch.long)
        with torch.no_grad():
            gen_batch = model.generate(batch_input, 5)
        print(f"âœ“ æ‰¹é‡ç”ŸæˆæˆåŠŸ! è¾“å‡ºå½¢çŠ¶: {gen_batch.shape}")
        assert gen_batch.shape[0] == 2, f"æœŸæœ›æ‰¹æ¬¡å¤§å°ä¸º 2ï¼Œå®é™…ä¸º {gen_batch.shape[0]}"
        
        print("\nğŸ‰ Transformer.generate å‡½æ•°è¾¹ç¼˜æƒ…å†µæµ‹è¯•å…¨éƒ¨é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"Transformer.generate å‡½æ•°è¾¹ç¼˜æƒ…å†µæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n========== è¿è¡Œ Transformer.generate å‡½æ•°æµ‹è¯• ==========")
       
    tests = [
        ("åŸºæœ¬åŠŸèƒ½æµ‹è¯•", test_generate_function),
        ("è¾¹ç¼˜æƒ…å†µæµ‹è¯•", test_generate_edge_cases)
    ]
    
    all_passed = True
    
    for test_name, test_func in tests:
        print(f"\n--- {test_name} ---")
        if not test_func():
            all_passed = False
    
    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼")


if __name__ == "__main__":
    run_all_tests()