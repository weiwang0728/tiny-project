import os
import sys

# ç¡®ä¿èƒ½å¯¼å…¥tokenizeræ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tokenizer import Tokenzier

def test_tokenizer():
    # æµ‹è¯•1: åˆå§‹åŒ–åˆ†è¯å™¨
    print("=== æµ‹è¯•1: åˆå§‹åŒ–åˆ†è¯å™¨ ===")
    try:
        tokenizer = Tokenzier()
        print("åˆ†è¯å™¨åˆå§‹åŒ–æˆåŠŸ!")
        print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.n_words}")
        print(f"BOS ID: {tokenizer.bos_id}")
        print(f"EOS ID: {tokenizer.eos_id}")
        print(f"PAD ID: {tokenizer.pad_id}")
    except Exception as e:
        print(f"åˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•2: åŸºæœ¬çš„ç¼–ç è§£ç åŠŸèƒ½
    print("\n=== æµ‹è¯•2: åŸºæœ¬çš„ç¼–ç è§£ç åŠŸèƒ½ ===")
    test_text = "Hello, world! This is a test."
    try:
        # ä¸å¸¦BOSå’ŒEOSçš„ç¼–ç 
        tokens = tokenizer.encode(test_text, bos=False, eos=False)
        print(f"åŸå§‹æ–‡æœ¬: {test_text}")
        print(f"ç¼–ç ç»“æœ: {tokens}")
        print(f"Tokenæ•°é‡: {len(tokens)}")
        
        # è§£ç å›æ–‡æœ¬
        decoded_text = tokenizer.decode(tokens)
        print(f"è§£ç ç»“æœ: {decoded_text}")
        
        # éªŒè¯ç¼–ç è§£ç çš„ä¸€è‡´æ€§
        assert decoded_text == test_text, f"ç¼–ç è§£ç ä¸ä¸€è‡´: æœŸæœ› '{test_text}', å¾—åˆ° '{decoded_text}'"
        print("âœ“ ç¼–ç è§£ç ä¸€è‡´æ€§éªŒè¯é€šè¿‡!")
    except Exception as e:
        print(f"åŸºæœ¬ç¼–ç è§£ç æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•3: å¸¦BOSå’ŒEOSçš„ç¼–ç 
    print("\n=== æµ‹è¯•3: å¸¦BOSå’ŒEOSçš„ç¼–ç  ===")
    try:
        tokens_with_bos_eos = tokenizer.encode(test_text, bos=True, eos=True)
        print(f"å¸¦BOSå’ŒEOSçš„ç¼–ç ç»“æœ: {tokens_with_bos_eos}")
        print(f"Tokenæ•°é‡: {len(tokens_with_bos_eos)}")
        
        # éªŒè¯BOSå’ŒEOSæ˜¯å¦æ­£ç¡®æ·»åŠ 
        assert tokens_with_bos_eos[0] == tokenizer.bos_id, "BOS IDæœªæ­£ç¡®æ·»åŠ "
        assert tokens_with_bos_eos[-1] == tokenizer.eos_id, "EOS IDæœªæ­£ç¡®æ·»åŠ "
        assert tokens_with_bos_eos[1:-1] == tokens, "ä¸­é—´çš„tokenä¸ä¸å¸¦BOS/EOSçš„ç»“æœä¸ä¸€è‡´"
        print("âœ“ BOSå’ŒEOSæ·»åŠ éªŒè¯é€šè¿‡!")
        
        # è§£ç å¸¦BOSå’ŒEOSçš„æ–‡æœ¬
        decoded_with_bos_eos = tokenizer.decode(tokens_with_bos_eos)
        print(f"å¸¦BOSå’ŒEOSçš„è§£ç ç»“æœ: '{decoded_with_bos_eos}'")
    except Exception as e:
        print(f"å¸¦BOSå’ŒEOSçš„ç¼–ç æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•4: ç‰¹æ®Šå­—ç¬¦å’Œé•¿æ–‡æœ¬
    print("\n=== æµ‹è¯•4: ç‰¹æ®Šå­—ç¬¦å’Œé•¿æ–‡æœ¬ ===")
    special_text = "ä½ å¥½ï¼Œä¸–ç•Œï¼12345 ğŸ˜Š ä¸­æ–‡æµ‹è¯• è‹±æ–‡æµ‹è¯• Special chars: !@#$%^&*()"
    long_text = "This is a longer text that tests how well the tokenizer handles multiple sentences. It includes various words and punctuation. Let's see how it performs." * 3
    
    try:
        # æµ‹è¯•ç‰¹æ®Šå­—ç¬¦
        special_tokens = tokenizer.encode(special_text, bos=False, eos=False)
        special_decoded = tokenizer.decode(special_tokens)
        print(f"ç‰¹æ®Šå­—ç¬¦æ–‡æœ¬: {special_text}")
        print(f"ç‰¹æ®Šå­—ç¬¦è§£ç ç»“æœ: {special_decoded}")
        print(f"ç‰¹æ®Šå­—ç¬¦Tokenæ•°é‡: {len(special_tokens)}")
        
        # æµ‹è¯•é•¿æ–‡æœ¬
        long_tokens = tokenizer.encode(long_text, bos=False, eos=False)
        long_decoded = tokenizer.decode(long_tokens)
        print(f"é•¿æ–‡æœ¬Tokenæ•°é‡: {len(long_tokens)}")
        print(f"é•¿æ–‡æœ¬è§£ç ç»“æœé•¿åº¦: {len(long_decoded)}")
        print(f"é•¿æ–‡æœ¬è§£ç å‰å‡ ä¸ªå­—ç¬¦: {long_decoded[:50]}...")
        
        print("âœ“ ç‰¹æ®Šå­—ç¬¦å’Œé•¿æ–‡æœ¬æµ‹è¯•é€šè¿‡!")
    except Exception as e:
        print(f"ç‰¹æ®Šå­—ç¬¦å’Œé•¿æ–‡æœ¬æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•5: ç©ºå­—ç¬¦ä¸²å¤„ç†
    print("\n=== æµ‹è¯•5: ç©ºå­—ç¬¦ä¸²å¤„ç† ===")
    try:
        empty_tokens = tokenizer.encode("", bos=True, eos=True)
        print(f"ç©ºå­—ç¬¦ä¸²å¸¦BOSå’ŒEOSçš„ç¼–ç : {empty_tokens}")
        assert len(empty_tokens) == 2, f"ç©ºå­—ç¬¦ä¸²å¸¦BOSå’ŒEOSåº”è¯¥åªæœ‰2ä¸ªtokenï¼Œå®é™…æœ‰{len(empty_tokens)}ä¸ª"
        assert empty_tokens[0] == tokenizer.bos_id and empty_tokens[1] == tokenizer.eos_id, "ç©ºå­—ç¬¦ä¸²çš„BOS/EOSä¸æ­£ç¡®"
        
        empty_decoded = tokenizer.decode(empty_tokens)
        print(f"ç©ºå­—ç¬¦ä¸²è§£ç ç»“æœ: '{empty_decoded}'")
        print("âœ“ ç©ºå­—ç¬¦ä¸²å¤„ç†æµ‹è¯•é€šè¿‡!")
    except Exception as e:
        print(f"ç©ºå­—ç¬¦ä¸²å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    print("\næ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    return True

if __name__ == "__main__":
    test_tokenizer()